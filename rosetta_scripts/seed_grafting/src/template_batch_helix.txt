#!/bin/bash
#SBATCH --nodes 1
#SBATCH --qos=serial
#SBATCH --ntasks-per-node 1
#SBATCH --cpus-per-task 1
#SBATCH --mem 4096
#SBATCH --time 12:00:00
#SBATCH --array=1-{seed_nr}
#SBATCH --output=exelogs/out/splitfile.%A_%a.out
#SBATCH --error=exelogs/err/splitfile.%A_%a.err

# Depending on your HPC system it is recommended to uncomment/adapt the following lines to avoid too many submitted jobs

#check_jobs () {{
#    n_jobs=$(squeue -u username -h -t pending,running -r | wc -l)
#    n_jobs_total=$(squeue -h -t pending,running -r | wc -l)
#    if [ $n_jobs -lt 8000 ] && [ $n_jobs_total -lt 29000 ] ; then
#        return 0
#    else
#        return 1
#    fi
#}}
#sleep $((5 + $RANDOM % 200))
#until check_jobs; do
#        sleep $((120 + $RANDOM % 120))
#        printf  "Too many jobs : waiting 2min \n"
#    done

MYID=$SLURM_ARRAY_TASK_ID
COMPLEX=$(sed -n "$MYID"p ./prep/inlist.txt)

python3 ./utils/extract_complex_helix.py $COMPLEX $MYID
python3 ./utils/crop_seed_helix.py $MYID

HOTSPOTS=$(python3 ./utils/find_hotspots.py $MYID)

echo $HOTSPOTS
mkdir -p out/seed_$MYID

sbatch ./prep/grafting_submitter.slurm $MYID $HOTSPOTS
